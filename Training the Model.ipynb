{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385c6d90-2145-4c25-bd07-59a5ee188546",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167c4599-641b-454e-9f80-69763e5e9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        drag_coefficient = torch.tensor(float(self.annotations.iloc[idx, 1]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, drag_coefficient\n",
    "\n",
    "# Step 2: Define a Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Step 4: Split the Dataset\n",
    "# Assume you have already split your dataset into train and validation sets\n",
    "\n",
    "# Step 5: Define Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Step 6: Choose an Optimizer\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693c77a-7128-4808-9ff2-e87aa0a64afb",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9368677c-1fbb-4d44-85c8-37b859698713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.0050\n",
      "Epoch [2/15], Loss: 0.0050\n",
      "Epoch [3/15], Loss: 0.0050\n",
      "Epoch [4/15], Loss: 0.0050\n",
      "Epoch [5/15], Loss: 0.0050\n",
      "Epoch [6/15], Loss: 0.0050\n",
      "Epoch [7/15], Loss: 0.0050\n",
      "Epoch [8/15], Loss: 0.0050\n",
      "Epoch [9/15], Loss: 0.0050\n",
      "Epoch [10/15], Loss: 0.0050\n",
      "Epoch [11/15], Loss: 0.0050\n",
      "Epoch [12/15], Loss: 0.0050\n",
      "Epoch [13/15], Loss: 0.0050\n",
      "Epoch [14/15], Loss: 0.0050\n",
      "Epoch [15/15], Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split the Dataset\n",
    "# Since you only have a training set, you can skip this step\n",
    "\n",
    "# Step 7: Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=15):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Step 8: Evaluation (Optional)\n",
    "# If you don't have a separate validation set, you can skip the evaluation step\n",
    "\n",
    "# Step 11: Putting It All Together\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = CarDataset(csv_file='norm dataset/annotations.csv', root_dir='norm dataset/edgelefts JPG', transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a240bd6d-7d6b-486e-a02a-423dffd59e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'engine.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f401d5-2458-4fec-889c-17e16105916c",
   "metadata": {},
   "source": [
    "# Loading in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb023ee-534f-4d79-9c15-90327e69e430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=32768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define your model architecture (the same as the one used during training)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('engine.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aed16a-fa44-4d9a-8540-9777f192e387",
   "metadata": {},
   "source": [
    "# Testing with an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c175fb6b-4d76-402f-b993-574939082416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted drag coefficient: 0.43892672657966614\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define a function to predict the drag coefficient\n",
    "def predict_drag_coefficient(image_path, model, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    predicted_coefficient = output.item()\n",
    "    return predicted_coefficient\n",
    "\n",
    "# Input a car outline image and get the predicted drag coefficient\n",
    "image_path = 'testing_normal/003_INVICTO_P_side-1200x801.jpg'\n",
    "predicted_coefficient = predict_drag_coefficient(image_path, model, test_transform)\n",
    "print(f'Predicted drag coefficient: {predicted_coefficient}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c907b-3908-4a93-881f-48f140085ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf4d53-589b-4e14-9480-4f868d24b450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
